{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read collected reddit dataset\n",
    "df = pd.read_csv(\"raw_reddit_news_posts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use reddit post title\n",
    "df = df[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude stopwords\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# exclude punctuation marks\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    # cast text to string type and lowercase\n",
    "    text = \" \".join([i for i in str(text).lower().split() if i not in stop])\n",
    "    \n",
    "    text = \"\".join(c for c in text if c not in punctuation)\n",
    "    \n",
    "    text = \" \".join(lemmatizer.lemmatize(w) for w in text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# clean text\n",
    "text = [clean_text(d).split() for d in df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(tokenizer=lambda d : d, lowercase=False)\n",
    "\n",
    "# count_vec = CountVectorizer(tokenizer=lambda d : d, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_out = tf_idf_vec.fit_transform(text)\n",
    "\n",
    "# count_vec_out = count_vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocabulary\n",
    "vocab = tf_idf_vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LatentDirichletAllocation(n_components=6, max_iter=10, random_state=20)\n",
    "\n",
    "topics = model.fit_transform(tf_idf_out)\n",
    "\n",
    "# get topic distribution\n",
    "topic_dist = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['u' 'china' 'say' 'russia']\n",
      "Topic 2: ['coronavirus' 'case' 'covid19' 'new']\n",
      "Topic 3: ['coronavirus' 'trump' 'test' 'china']\n",
      "Topic 4: ['ukraine' 'u' 'russian' 'russia']\n",
      "Topic 5: ['vaccine' 'covid19' 'covid' 'coronavirus']\n",
      "Topic 6: ['china' 'u' 'climate' 'say']\n"
     ]
    }
   ],
   "source": [
    "# number of topics\n",
    "n = 5\n",
    "\n",
    "for i, td in enumerate(topic_dist):\n",
    "    sorted_topic_indices = np.argsort(td)\n",
    "    \n",
    "    topic_words = np.array(vocab)[sorted_topic_indices]\n",
    "    \n",
    "    topic_words = topic_words[:-n:-1]\n",
    "    \n",
    "    print(f\"Topic {i+1}: {topic_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Topic: 1\n",
      "Document 2: Topic: 2\n",
      "Document 3: Topic: 5\n",
      "Document 4: Topic: 5\n",
      "Document 5: Topic: 2\n",
      "Document 6: Topic: 4\n",
      "Document 7: Topic: 3\n",
      "Document 8: Topic: 3\n",
      "Document 9: Topic: 3\n",
      "Document 10: Topic: 1\n"
     ]
    }
   ],
   "source": [
    "d_topic = model.transform(tf_idf_out)\n",
    "\n",
    "for i in range(d_topic[:10, :].shape[0]):\n",
    "    topic_d = d_topic[i].argmax()\n",
    "    \n",
    "    print(f\"Document {i+1}: Topic: {topic_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

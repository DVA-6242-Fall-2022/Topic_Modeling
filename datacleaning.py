# -*- coding: utf-8 -*-
"""DataCleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sDbzyXMEWLUVCOODB_oRbmqvcTGp2Ek2
"""

import pandas as pd
import numpy as np
import gensim
from nltk.stem import WordNetLemmatizer, SnowballStemmer
np.random.seed(10)
import nltk
import re
import plotly.express as px
nltk.download('wordnet')
nltk.download('omw-1.4')

def clean_text(text):
    text = text.lower() # lowercase everything
    text = text.encode('ascii', 'ignore').decode()  # remove unicode characters
    text = re.sub(r'https*\S+', ' ', text) # remove links
    text = re.sub(r'http*\S+', ' ', text)
    # cleaning up text
    text = re.sub(r'\'\w+', '', text) 
    text = re.sub(r'\w*\d+\w*', '', text)
    text = re.sub(r'\s{2,}', ' ', text)
    text = re.sub(r'\s[^\w\s]\s', '', text)
    text = re.sub("[\(\[].*?[\)\]]", "", text)
    text = re.sub('[^a-zA-Z0-9 \.]', '', text)
    return text

def lemmatize(text):
  return WordNetLemmatizer().lemmatize(text, pos='v')
def preprocess(text):
    if type(text)!=str:
      return ''
    result = []
    text = text.replace('[removed]', '').replace('[deleted]', '')
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:
            result.append(lemmatize(token))
    return ' '.join(result)

file_name = "/content/drive/MyDrive/Reddit_ToT/raw_reddit_movies_posts_comments.csv"
df = pd.read_csv(file_name)
df.fillna("", inplace=True)

## Cleaning the comments and title
df['cleaned_comments'] = df['cleaned_comments'].map(preprocess)
df['cleaned_title'] = df.title.apply(clean_text)
df.to_csv("/content/drive/MyDrive/Reddit_ToT/raw_reddit_movies_posts_comments.csv")